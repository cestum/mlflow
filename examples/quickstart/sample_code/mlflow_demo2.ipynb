{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mflow Demonstration 2\n",
    "\n",
    "Running this notebook takes about 6-1/2 minutes on a Mid-2014 MacBook Pro\n",
    "\n",
    "### Data Scientist Responsibility\n",
    "This notebook demonstrates performing hyper-parameter optimization using randomized cross-validation on several algorithms, which conforms to the scikit-learn learn api structure, and using mlflow to record the optimal hyper-parameters, model training results and data sets used in training.\n",
    "\n",
    "The algorithms used are\n",
    "* sklearn ElasticNet\n",
    "* sklearn RandomForestRegressor\n",
    "* sklearn ExtraTreesRegressor\n",
    "* sklearn MLPRegressor\n",
    "* xgboost XGBRegressor\n",
    "\n",
    "Each algorithm is executed as a mlflow Run.\n",
    "\n",
    "The train/test data sets used in a Run are saved as an mlflow artifact.\n",
    "\n",
    "The mlflow experiment is organized into 3 sets of runs.  Each run set trains the algorithms on a specific format of the training data:\n",
    "* raw values\n",
    "* Values are standardized: mean centered / scaled by standard deviaion\n",
    "* Values are transformed to range $[0, 1]$\n",
    "\n",
    "### mlflow Functionality\n",
    "The mlflow tracking server is used to record experimental results: hyper-parameters, metrics and modeling artifacts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os.path\n",
    "import os\n",
    "import socket\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import pickle\n",
    "import shutil\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import mlflow\n",
    "import mlflow.sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3673, 11)\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "# Read the wine-quality csv file (make sure you're running this from the root of MLflow!)\n",
    "data = pd.read_csv(\"./wine-quality.csv\")\n",
    "\n",
    "# Split the data into training and test sets. (0.75, 0.25) split.\n",
    "train, test = train_test_split(data,random_state=13)\n",
    "\n",
    "# The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "train_x = train.drop([\"quality\"], axis=1)\n",
    "test_x = test.drop([\"quality\"], axis=1)\n",
    "train_y = train[[\"quality\"]]\n",
    "test_y = test[[\"quality\"]]\n",
    "\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Custom Transformer class to return values of dataframe as numpy array\n",
    "###\n",
    "class NullTransformer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        return X.values\n",
    "\n",
    "data_sets = []\n",
    "    \n",
    "# create numpy arrays for the different data set formats\n",
    "for data_set_type, TransformerClass in [('raw values',NullTransformer), \n",
    "                                        ('standardized values', StandardScaler), \n",
    "                                        ('Min/Max values', MinMaxScaler)]:\n",
    "    # set up transformer\n",
    "    this_transformer = TransformerClass()\n",
    "    \n",
    "    # fit transform parameters\n",
    "    #this_transformer.fit(train_x)\n",
    "    \n",
    "    # add transformed training data\n",
    "    data_sets.append({\n",
    "        'label': data_set_type,\n",
    "        'train_x': this_transformer.fit_transform(train_x),\n",
    "        'test_x': this_transformer.transform(test_x),\n",
    "        'train_y': train_y,\n",
    "        'test_y': test_y\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data set type: raw values \n",
      "sample training values:\n",
      " [[ 7.2    0.24   0.24   1.7    0.045]\n",
      " [ 7.6    0.33   0.41  13.7    0.045]\n",
      " [ 7.     0.27   0.32   6.8    0.047]\n",
      " [ 7.2    0.2    0.34   2.7    0.032]\n",
      " [ 5.7    0.255  0.65   1.2    0.079]] \n",
      "\n",
      "data set type: standardized values \n",
      "sample training values:\n",
      " [[ 0.39624274 -0.37825504 -0.80060088 -0.9399674  -0.02735814]\n",
      " [ 0.87268093  0.53201882  0.62459925  1.46345779 -0.02735814]\n",
      " [ 0.15802364 -0.07483042 -0.12991847  0.08148831  0.06784456]\n",
      " [ 0.39624274 -0.7828212   0.03775213 -0.73968196 -0.64617568]\n",
      " [-1.39040049 -0.22654273  2.63664649 -1.04011011  1.59108772]] \n",
      "\n",
      "data set type: Min/Max values \n",
      "sample training values:\n",
      " [[0.3        0.15686275 0.19512195 0.03548387 0.11870504]\n",
      " [0.34       0.24509804 0.33333333 0.42258065 0.11870504]\n",
      " [0.28       0.18627451 0.2601626  0.2        0.12589928]\n",
      " [0.3        0.11764706 0.27642276 0.06774194 0.07194245]\n",
      " [0.15       0.17156863 0.52845528 0.01935484 0.24100719]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in data_sets:\n",
    "    print('data set type:',d['label'],\"\\nsample training values:\\n\",d['train_x'][:5,:5],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary directory to hold artifacts for logging\n",
    "tempdir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to mlflow tracking server to record model training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://mlflow_tracker:5000'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assumes MLFLOW_TRACKING_URI environment variable is set\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for mlflow experiment and randomized cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "experiment_id = mlflow.set_experiment('mlflow_demo2')\n",
    "\n",
    "# work-around for issue with recording first run of an experiment Issue #852\n",
    "with mlflow.start_run(experiment_id=experiment_id):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up for randomized cv search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scorer r2\n",
      "n_jobs -1\n",
      "verbose 1\n",
      "n_iter 10\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "with open('./trainer.yaml','r') as f:\n",
    "    parms = yaml.safe_load(f.read())\n",
    "\n",
    "#%%\n",
    "for p in parms['global-run-time']:\n",
    "    print(p,parms['global-run-time'][p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start run>>> raw values model1 from sklearn.linear_model import ElasticNet {'alpha': 'scipy.stats.uniform(0,1)', 'l1_ratio': 'scipy.stats.uniform(0,1)', 'random_state': '[13]'} \n",
      "\n",
      "for model:  model1 n_jobs:  -1\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  50 | elapsed:    1.7s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====> model1  score: 0.26206663836386285  optimal hyper-parms: {'alpha': 0.009412770080969413, 'l1_ratio': 0.35833378270496974, 'random_state': 13} \n",
      "\n",
      "\n",
      "start run>>> raw values model2 from sklearn.ensemble import RandomForestRegressor {'n_estimators': 'scipy.stats.randint(50,1000)', 'max_depth': 'scipy.stats.randint(1,10)', 'random_state': '[13]', 'max_features': 'scipy.stats.randint(5,11)'} \n",
      "\n",
      "for model:  model2 n_jobs:  -1\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   15.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====> model2  score: 0.42315386291103835  optimal hyper-parms: {'max_depth': 9, 'max_features': 9, 'n_estimators': 364, 'random_state': 13} \n",
      "\n",
      "\n",
      "start run>>> raw values model3 from xgboost import XGBRegressor {'n_estimators': 'scipy.stats.randint(50,1000)', 'max_depth': 'scipy.stats.randint(1,10)', 'learning_rate': 'scipy.stats.uniform(0,1)', 'random_state': '[13]'} \n",
      "\n",
      "for model:  model3 n_jobs:  -1\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   16.8s finished\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====> model3  score: 0.4471143248773945  optimal hyper-parms: {'learning_rate': 0.058512491882074746, 'max_depth': 6, 'n_estimators': 453, 'random_state': 13} \n",
      "\n",
      "\n",
      "start run>>> standardized values model1 from sklearn.linear_model import ElasticNet {'alpha': 'scipy.stats.uniform(0,1)', 'l1_ratio': 'scipy.stats.uniform(0,1)', 'random_state': '[13]'} \n",
      "\n",
      "for model:  model1 n_jobs:  -1\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "====> model1  score: 0.28397785263783276  optimal hyper-parms: {'alpha': 0.009412770080969413, 'l1_ratio': 0.35833378270496974, 'random_state': 13} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start run>>> standardized values model2 from sklearn.ensemble import RandomForestRegressor {'n_estimators': 'scipy.stats.randint(50,1000)', 'max_depth': 'scipy.stats.randint(1,10)', 'random_state': '[13]', 'max_features': 'scipy.stats.randint(5,11)'} \n",
      "\n",
      "for model:  model2 n_jobs:  -1\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   17.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====> model2  score: 0.4231431781541996  optimal hyper-parms: {'max_depth': 9, 'max_features': 9, 'n_estimators': 364, 'random_state': 13} \n",
      "\n",
      "\n",
      "start run>>> standardized values model3 from xgboost import XGBRegressor {'n_estimators': 'scipy.stats.randint(50,1000)', 'max_depth': 'scipy.stats.randint(1,10)', 'learning_rate': 'scipy.stats.uniform(0,1)', 'random_state': '[13]'} \n",
      "\n",
      "for model:  model3 n_jobs:  -1\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   17.2s finished\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====> model3  score: 0.4458905296678439  optimal hyper-parms: {'learning_rate': 0.058512491882074746, 'max_depth': 6, 'n_estimators': 453, 'random_state': 13} \n",
      "\n",
      "\n",
      "start run>>> Min/Max values model1 from sklearn.linear_model import ElasticNet {'alpha': 'scipy.stats.uniform(0,1)', 'l1_ratio': 'scipy.stats.uniform(0,1)', 'random_state': '[13]'} \n",
      "\n",
      "for model:  model1 n_jobs:  -1\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====> model1  score: 0.24571762805205616  optimal hyper-parms: {'alpha': 0.009412770080969413, 'l1_ratio': 0.35833378270496974, 'random_state': 13} \n",
      "\n",
      "\n",
      "start run>>> Min/Max values model2 from sklearn.ensemble import RandomForestRegressor {'n_estimators': 'scipy.stats.randint(50,1000)', 'max_depth': 'scipy.stats.randint(1,10)', 'random_state': '[13]', 'max_features': 'scipy.stats.randint(5,11)'} \n",
      "\n",
      "for model:  model2 n_jobs:  -1\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   16.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====> model2  score: 0.423286748241008  optimal hyper-parms: {'max_depth': 9, 'max_features': 9, 'n_estimators': 364, 'random_state': 13} \n",
      "\n",
      "\n",
      "start run>>> Min/Max values model3 from xgboost import XGBRegressor {'n_estimators': 'scipy.stats.randint(50,1000)', 'max_depth': 'scipy.stats.randint(1,10)', 'learning_rate': 'scipy.stats.uniform(0,1)', 'random_state': '[13]'} \n",
      "\n",
      "for model:  model3 n_jobs:  -1\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   16.7s finished\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====> model3  score: 0.44714936649161907  optimal hyper-parms: {'learning_rate': 0.058512491882074746, 'max_depth': 6, 'n_estimators': 453, 'random_state': 13} \n",
      "\n",
      "<<<<ALL DONE>>>>>\n"
     ]
    }
   ],
   "source": [
    "# for each data set perform randomized cross-validation hyper-parameter tuning\n",
    "for data in data_sets:\n",
    "    \n",
    "    for model in parms['models']:\n",
    "\n",
    "        model_id = list(model.keys())[0]\n",
    "        model_specs = model[model_id]\n",
    "        print(\"\\nstart run>>>\",data['label'],model_id, model_specs['estimator'], model_specs['hyper-parameters'],'\\n')\n",
    "\n",
    "        with mlflow.start_run(experiment_id = experiment_id):\n",
    "\n",
    "            # turn hyper-parameter specificaitons into runable form\n",
    "            runable_hyper_parameters = {}\n",
    "            for k in model_specs['hyper-parameters'].keys():\n",
    "                runable_hyper_parameters[k] = eval(model_specs['hyper-parameters'][k])\n",
    "\n",
    "            #  Now do randomized hyper-parameter tuning\n",
    "            estimator_string = model_specs['estimator'] + ' as ThisEstimator'\n",
    "            exec(estimator_string)\n",
    "            this_model = ThisEstimator()\n",
    "\n",
    "            # determine degree of parallelism\n",
    "            try:\n",
    "                n_jobs = model_specs['model-run-time']['n_jobs']\n",
    "            except:\n",
    "                n_jobs = parms['global-run-time']['n_jobs']\n",
    "\n",
    "            print('for model: ', model_id,\"n_jobs: \",n_jobs)\n",
    "\n",
    "            randomized_search_cv = RandomizedSearchCV(this_model,\n",
    "                                                      param_distributions = runable_hyper_parameters,\n",
    "                                                      n_iter=parms['global-run-time']['n_iter'],\n",
    "                                                      cv=5,\n",
    "                                                      n_jobs=n_jobs,\n",
    "                                                      scoring=parms['global-run-time']['scorer'],\n",
    "                                                      random_state=13,\n",
    "                                                      verbose=parms['global-run-time']['verbose'])\n",
    "            \n",
    "            randomized_search_cv.fit(data['train_x'], data['train_y'].quality)\n",
    "            \n",
    "            print(\"\\n====>\",model_id, ' score:', randomized_search_cv.best_score_, ' optimal hyper-parms:' ,randomized_search_cv.best_params_,'\\n')\n",
    "\n",
    "            # log training data\n",
    "            with open(os.path.join(tempdir,'data.pkl'),'wb') as t:\n",
    "                pickle.dump({'train_x': data['train_x'], 'train_y': data['train_y'], \n",
    "                         'test_x': data['test_x'], 'test_y': data['test_y']},t)\n",
    "\n",
    "            mlflow.log_artifact(os.path.join(tempdir,'data.pkl'),'data_sets')\n",
    "\n",
    "            # record results of training in mlflow\n",
    "            test_score = randomized_search_cv.best_estimator_.score(data['test_x'], data['test_y'].quality)\n",
    "            mlflow.log_metric(parms['global-run-time']['scorer'],test_score)\n",
    "\n",
    "            # record algorithm parameters\n",
    "            for k in randomized_search_cv.best_params_.keys():\n",
    "                mlflow.log_param(k,randomized_search_cv.best_params_[k])\n",
    "                \n",
    "            \n",
    "            # record other parameters\n",
    "            mlflow.log_param('data_set_type',data['label'])\n",
    "            mlflow.log_param('algorithm',model_specs['estimator'].split(' ')[3])\n",
    "\n",
    "            mlflow.sklearn.log_model(randomized_search_cv.best_estimator_,'best_estimator')\n",
    "\n",
    "            mlflow.set_tag('estimator',model_specs['estimator'])\n",
    "            mlflow.set_tag('data_set_type',data['label'])\n",
    "\n",
    "            # clean up\n",
    "            del(this_model,ThisEstimator,randomized_search_cv)\n",
    "        \n",
    "print('<<<<ALL DONE>>>>>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up temporary directory\n",
    "shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
